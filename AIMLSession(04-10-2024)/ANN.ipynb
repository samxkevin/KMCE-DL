{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9b2cb8",
   "metadata": {},
   "source": [
    "# Deep Learning Fundamentals: Forward and Backward Propagation\n",
    "\n",
    "This notebook demonstrates the basics of deep learning, including forward propagation, loss calculation, and backpropagation. Lets build a simple neural network from scratch using NumPy to grasp the underlying concepts without relying on high-level libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e3036",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We'll use NumPy for numerical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbd305",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1018496144.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install numpy\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The Whole world is an equation. We have a variables,weights(Coefficients of the variable) and Linearity(Degree of an equation) and the result of the relation of all \n",
    "these things together. AI is the thing which is parametrizing everything in the world. Visualize these variables, weights as a matrix ad add the Linearity to it.\n",
    "Which will add us to a sensible equation. GPU's are fast matrix calculators so they are being used in the field of AI. Seamingly unconnected stuff is not really \n",
    "unconnected until we have an equation. The world is going to a field called Hypercustomization.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Andrej Karparthy: He was the co-founder of open-AI. He was the CTO of AI division of Tesla. He made a series of zero to hero. It is worth it.\n",
    "ANN- Artificial Neural Network. Hidden Layers are also called as Black-Box. Gaussian Distribution. Probablity Distribution Function, Derivatives important topics\n",
    "in this case.  If we have to be great in AI we have to be intimate with maths. We have to improve the grounding in math. Intractable Problem: y has no relationship with x.\n",
    "We have an assumed function and training data for it. give input for that. compare that with the actual results. There will be errors. With that error, change the \n",
    "parameters of the assumed function. Repeat this for accurate and precise results(EPOC).\n",
    "\"\"\"\n",
    "\"\"\"Jupyter notebook is a python interpretor which is widely used in Deep Learning Applications. Loss Function- We will never get correct output.Initiallly we do not know the ways so we start with random weights which result in y now we give that x and predict that y which is in lot of variation with ground one. The difference between the actual value and the predicted value for y is called the Loss Variable. We use cross entropy for benign or malignity of the function for classification of the error.\n",
    "AI is an attempt to decrease the loss function to a bare minimum.\"\"\"\n",
    "\n",
    "\"\"\" pip install numpy\n",
    "pip install micrograd\n",
    "pip install matplotlib \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b8f93",
   "metadata": {},
   "source": [
    "Execute the above commands in your shell to install the requirements for this Lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a8430",
   "metadata": {},
   "source": [
    "## Step 2: Define Activation Functions\n",
    "\n",
    "We'll use the sigmoid activation function and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9764d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10288e66",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Parameters\n",
    "\n",
    "We'll set up input features, target outputs, and initialize weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f17137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data (features) and true label\n",
    "x = np.array([\n",
    "    [0.5, 1.5],\n",
    "    [1.0, 2.0],\n",
    "    [1.5, 0.5],\n",
    "    [3.0, 1.0]\n",
    "])  # Input: 4 samples, 2 features each\n",
    "y_true = np.array([[0], [0], [1], [1]])  # True output labels\n",
    "\n",
    "# Initialize weights and bias randomly\n",
    "np.random.seed(0)  # For reproducibility\n",
    "w = np.random.randn(2, 1)  # Weights: 2 inputs to 1 output neuron\n",
    "b = np.random.randn(1)     # Bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc486d",
   "metadata": {},
   "source": [
    "## Step 4: Forward Propagation\n",
    "\n",
    "Compute the outputs of the hidden and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd742dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "z = np.dot(x, w) + b                # Linear combination\n",
    "y_pred = sigmoid(z)    # Sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b8874",
   "metadata": {},
   "source": [
    "## Step 5: Compute Loss\n",
    "\n",
    "We'll use Mean Squared Error (MSE) to measure the difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c49987",
   "metadata": {},
   "source": [
    "## Step 6: Backward Propagation\n",
    "\n",
    "Calculate gradients and update weights and biases accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss = 2 * (y_pred - y_true) / y_pred.size          # Derivative of loss w.r.t y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f154f0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial l}{\\partial y_p} = \\frac{2}{n}(y_p - y_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d262098",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_activation = sigmoid_derivative(y_pred)             # Derivative of sigmoid activation: sigmoid(z)[1-sigmoid(z)]\n",
    "d_z = d_loss * d_activation                           # Chain rule: derivative w.r.t z\n",
    "d_w = np.dot(x.T, d_z) / x.shape[0]                   # Gradient w.r.t weights\n",
    "d_b = np.mean(d_z, axis=0)                            # Gradient w.r.t bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d5df2",
   "metadata": {},
   "source": [
    "## Step 7: Update weights and bias and show the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f5b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights:\n",
      " [[1.75704588]\n",
      " [0.38350287]]\n",
      "Updated bias:\n",
      " [0.96932077]\n",
      "Loss: 0.4484838343811475\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1 #learning rate\n",
    "w -= learning_rate * d_w\n",
    "b -= learning_rate * d_b\n",
    "\n",
    "print(\"Updated weights:\\n\", w)\n",
    "print(\"Updated bias:\\n\", b)\n",
    "print(\"Loss:\", loss)\n",
    "#standard gradient Descent(1/m(sigma(yp-yt)^2))(stable, but time taking)\n",
    "#stocastic gradient descent one sample at a time(too many oscillation, fluctuation)(no sigma and 1/m)\n",
    "#mini-batch gradient descent(grouping the batches)(no. of batches=samples/2^8, no. of normalizations that a neural network undergoes during loss computation)\n",
    "#momentum gradient descent: previous gradient descents impact the present gradient descents\n",
    "#now learning rate also changes for different weights different learning rates\n",
    "#rms prop: (for learning rate decays)\n",
    "#optimizer: Adam optimizer next session\n",
    "#alpha is the learning rate\n",
    "#AdaGrad and RmsProp combined we get ADAM optimizer to get adaptive to both sparse and dense gradients\n",
    "#ADAM optimizer- Adaptive Moment Estimation\n",
    "#m: gradient 1st moment \n",
    "#v: squared gradient 2nd moment\n",
    "#alp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
